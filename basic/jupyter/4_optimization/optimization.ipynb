{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差最小化と最適化\n",
    "\n",
    "機械学習(特に教師あり学習)の目的は、パラメーターを調整して予測値 $\\hat{y}$ を正解 $y$ に近づけること(尤度最大化)や予測値 $\\hat{y}$ と正解 $y$ との差を小さくすること(誤差最小化)。尤度を最大化するのが計算的に難しい場合もあり、誤差最小化の問題として解いてパラメーターを求めることが多い。この尤度最大化や誤差最小化でパラメーターを求めることを最適化という。\n",
    "\n",
    "誤差を単純に正解と予測の差$\\left(y-\\hat{y}\\right)$とすると、負の値を取りうるので最小化できない。正解 $y$ と予測値 $\\hat{y}$ から誤差を求める方法はいくつかある。この誤差(Error)を求める関数のことをコスト関数(Cost function)・損失関数(Loss function)・目的関数(Objective function)などと呼ぶ。(厳密にはそれぞれ違うものを指している場合もある)\n",
    "\n",
    "## 平均二乗誤差(Mean Squared Error, MSE)\n",
    "\n",
    "回帰問題の誤差として最も使われる関数。正解 $y$ に含まれるノイズがガウス分布に従う場合に正しい選択だが、世の中の大抵のノイズはガウス分布に従うのでほとんどの場面で使える。\n",
    "\n",
    "$\\begin{eqnarray}MSE=\\frac{1}{n}\\sum_{i=1}^n{\\left(y_i-\\hat{y}_i\\right)^2}\\end{eqnarray}$\n",
    "\n",
    "pythonのコードで表すと\n",
    "\n",
    "> ```python\n",
    "> # n: サンプル(学習)データの数\n",
    "> # y: 正解データ(i番目のサンプルの正解がy[i])\n",
    "> # y_hat: 予測値(i番目のサンプルの予測値がy_hat[i])\n",
    "> mse = np.power(y - y_hat, 2).sum() / n\n",
    "> ```\n",
    "\n",
    "差を二乗しているので、正解と予測値の差が最も小さいところが最小誤差になる。\n",
    "\n",
    "## 交差エントロピー誤差(Cross-entropy Error)\n",
    "\n",
    "分類問題の誤差としてよく使われる関数。分類の問題を確率回帰の問題に捉え直して誤差を計算している。\n",
    "\n",
    "$\\begin{eqnarray}CrossEntropy=\\sum_{i=1}^n\\left[-y_ilog\\left(\\hat{y}_i\\right)-\\left(1-y_i\\right)log\\left(1-\\hat{y}_i\\right)\\right]\\end{eqnarray}$\n",
    "\n",
    "pythonのコードで表すと\n",
    "\n",
    "> ```python\n",
    "> # 二値分類の場合、正解ラベルは0か1で表現\n",
    "> # 予測は正解ラベル1のクラスに属する確率で表現\n",
    "> # 多クラス分類問題の正解ラベルはone-hot encodingで表現\n",
    "> # 例えば正解ラベルが5クラスあって、あるサンプルの正解がクラス3なら[0, 0, 1, 0, 0]を渡す\n",
    "> # 予測は各クラスへの所属確率なので、それぞれ0~1の範囲の値を取り、全クラスの予測を合計すると1になる\n",
    "> # 例：[0.01, 0.04, 0.9, 0.03, 0.02]\n",
    "> cross_entropy = (- y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)) # 2クラス\n",
    "> cross_entropy = (- y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).sum(axis=1) # 多クラス\n",
    "> ```\n",
    "\n",
    "### 導出\n",
    "\n",
    "高校生レベルの数学で導出できる。\n",
    "\n",
    "入力を $x$、現在のパラメーターを $\\theta$、正解ラベルを $y$、予測値を $\\hat{y}=\\phi\\left(x,\\ \\theta\\right)$、予測値が正解ラベルに属する確率を $p\\left(y\\ |\\ x,\\ \\theta\\right)$ とする。例えば、$\\hat{y}=\\phi\\left(x,\\ \\theta\\right)=0.9$ だとして $y=1$ のとき $p\\left(y\\ |\\ x,\\ \\theta\\right)=0.9$、$y=0$のとき $p\\left(y\\ |\\ x,\\ \\theta\\right)=0.1$。そして最大化したい尤度 $L$ を以下のように定義する。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "    L&=&p\\left(y_1\\ |\\ x_1,\\ \\theta_1\\right)\\times p\\left(y_2\\ |\\ x_2,\\ \\theta_2\\right)...p\\left(y_n\\ |\\ x_n,\\ \\theta_n\\right) \\\\\n",
    "    &=&\\prod_{i=1}^n{p\\left(y_i\\ |\\ x_i,\\ \\theta_i\\right)}\n",
    "\\end{eqnarray}$\n",
    "\n",
    "正解ラベル $y$ は$0$か$1$しか取らないので\n",
    "\n",
    "$p\\left(y_i\\ |\\ x_i,\\ \\theta_i\\right)=\\begin{cases}\n",
    "    \\hat{y}_i & (y=1) \\\\\n",
    "    1-\\hat{y}_i & (y=0)\n",
    "\\end{cases}$\n",
    "\n",
    "これを利用すると、以下のように変形できる。\n",
    "\n",
    "$\\begin{eqnarray}L=\\prod_{i=1}^n{\\left(\\hat{y}_i\\right)^{y_i}\\left(1-\\hat{y}_i\\right)^{1-y_i}}\\end{eqnarray}$\n",
    "\n",
    "確率の積よりも、その対数尤度(log-likelihood) $l$ の和にしたほうが扱いやすい。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "    l&=&logL \\\\\n",
    "    &=&\\sum_{i=1}^n\\left[y_ilog\\left(\\hat{y}_i\\right)+\\left(1-y_i\\right)log\\left(1-\\hat{y}_i\\right)\\right]\n",
    "\\end{eqnarray}$\n",
    "\n",
    "符合を反転させると、最小化したい誤差になる\n",
    "\n",
    "$\\begin{eqnarray}CrossEntropy=\\sum_{i=1}^n\\left[-y_ilog\\left(\\hat{y}_i\\right)-\\left(1-y_i\\right)log\\left(1-\\hat{y}_i\\right)\\right]\\end{eqnarray}$\n",
    "\n",
    "### グラフ\n",
    "\n",
    "正解に近いと誤差が0に近くなり、遠いと誤差が無限大に近づいていく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = 1e-8\n",
    "y_hat = np.linspace(eps, 1 - eps, 200)\n",
    "cost_pos = - np.log(y_hat)\n",
    "cost_neg = - np.log(1 - y_hat)\n",
    "\n",
    "plt.plot(y_hat, cost_pos, label='y = 1')\n",
    "plt.plot(y_hat, cost_neg, label='y = 0')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('cost')\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配降下法(Gradient Descent)\n",
    "\n",
    "勾配降下法は最適化手法の1つで、コスト関数をパラメーター $\\theta$ についての関数 $J(\\theta)$ とみなし、$J(\\theta)$ の勾配を下っていく( $J(\\theta)$ の傾きと逆に進む)ことでコスト関数の最小値を目指す。(図は説明のためのもので、計算式は実際の勾配降下法とは異なる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J(theta):\n",
    "    return theta ** 2\n",
    "\n",
    "def J_grad(theta):\n",
    "    return theta * 2\n",
    "\n",
    "def gradient_descent(theta, alpha, n_step):\n",
    "    thetas = np.zeros(n_step)\n",
    "    costs = np.zeros(n_step)\n",
    "    grads = np.zeros((n_step, 2))\n",
    "\n",
    "    for step in range(n_step):\n",
    "        thetas[step] = theta\n",
    "        costs[step] = J(theta)\n",
    "        grad = alpha * J_grad(theta)\n",
    "        theta -= grad\n",
    "        grads[step] = [- grad, J(theta) - J(thetas[step])]\n",
    "\n",
    "    return (thetas, costs, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = .3 # 学習率\n",
    "theta = 9 # パラメーターの初期値\n",
    "n_step = 3 # パラメーターの更新回数\n",
    "\n",
    "plt.title('Concept of gradient descent 1D')\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = J(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "thetas, costs, grads = gradient_descent(theta, alpha, n_step)\n",
    "plt.quiver(thetas, costs, grads[:, 0], grads[:, 1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('cost')\n",
    "plt.xlim(x.min() - 5, x.max() + 5)\n",
    "plt.ylim(y.min() - 5, y.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "alpha = .3\n",
    "theta = np.array([9, 9])\n",
    "n_step = 3\n",
    "\n",
    "def J_2d(theta):\n",
    "    return np.power(theta, 2).sum()\n",
    "\n",
    "def J_grad_2d(theta):\n",
    "    return theta * 2\n",
    "\n",
    "def gradient_descent_2d(theta, alpha, n_step):\n",
    "    thetas = np.zeros(tuple([n_step]) + theta.shape)\n",
    "    costs = np.zeros(n_step)\n",
    "    grads = np.zeros((n_step, len(theta) + 1))\n",
    "    \n",
    "    for step in range(n_step):\n",
    "        thetas[step] = theta\n",
    "        costs[step] = J_2d(theta)\n",
    "        grad = alpha * J_grad_2d(theta)\n",
    "        theta = theta - grad\n",
    "        grads[step] = np.concatenate((- grad, np.array([J_2d(theta) - J_2d(thetas[step])])))\n",
    "    \n",
    "    return (thetas, costs, grads)\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = np.linspace(-10, 10, 30)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "z = np.power(grid, 2).sum(axis=1).reshape(xx.shape)\n",
    "\n",
    "x_min, x_max = x.min(), x.max()\n",
    "y_min, y_max = y.min(), y.max()\n",
    "z_min, z_max = z.min() - 20, z.max()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, elev=25, azim=-75)\n",
    "\n",
    "ax.set_title('Concept of gradient descent 2D')\n",
    "\n",
    "ax.plot_wireframe(xx, yy, z, rstride=2, cstride=2, alpha=.3)\n",
    "ax.contour(xx, yy, z, zdir='z', offset=z_min)\n",
    "\n",
    "thetas, costs, grads = gradient_descent_2d(theta, alpha, n_step)\n",
    "\n",
    "for qx, qy, qz, qu, qv, qw in zip(thetas[:, 0], thetas[:, 1], costs, grads[:, 0], grads[:, 1], grads[:, 2]):\n",
    "    length = np.sqrt(np.power([qu, qv, qw], 2).sum())\n",
    "    ax.quiver(qx, qy, qz, qu, qv, qw, pivot='tail', length=length, arrow_length_ratio=min(.3, 3.0 / length), color='red')\n",
    "\n",
    "x_flat = np.append(thetas[:, 0], thetas[-1, 0] + grads[-1, 0])\n",
    "y_flat = np.append(thetas[:, 1], thetas[-1, 1] + grads[-1, 1])\n",
    "z_flat = [z_min] * (n_step + 1)\n",
    "ax.plot(x_flat, y_flat, z_flat, marker='x', color='red')\n",
    "\n",
    "ax.set_xlabel('$\\\\theta_1$')\n",
    "ax.set_ylabel('$\\\\theta_2$')\n",
    "ax.set_zlabel('cost')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_zlim(z_min, z_max)\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_zticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率 $\\alpha$ は大きすぎるとコストが発散してしまい、小さすぎると収束に時間がかかりすぎる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw(ax, theta, alpha, n_step):\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    thetas, costs, grads = gradient_descent(theta, alpha, n_step)\n",
    "    ax.quiver(thetas, costs, grads[:, 0], grads[:, 1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "\n",
    "    ax.set_xlim(x.min() - 5, x.max() + 5)\n",
    "    ax.set_ylim(y.min() - 5, y.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = J(x)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Too large')\n",
    "draw(ax, 4, 1.1, 5)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Too small')\n",
    "draw(ax, 9, .03, 10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
