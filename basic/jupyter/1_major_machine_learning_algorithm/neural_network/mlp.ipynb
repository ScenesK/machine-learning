{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多層パーセプトロン(Multi-layer Perceptron)\n",
    "\n",
    "## 多層パーセプトロンとは\n",
    "\n",
    "- 単純パーセプトロンやロジスティック回帰のようなニューラルネットワークの入力と出力の間に中間層(隠れ層)を追加したもの\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "### データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "n_class = 3\n",
    "n_side_cluster = 2\n",
    "n_blob_sample = 100\n",
    "batch_size = 100\n",
    "n_step = 20000\n",
    "n_epoch = (batch_size * n_step) // (n_blob_sample * n_side_cluster ** 2)\n",
    "\n",
    "for row, center_v in enumerate(np.linspace(-2, 2, n_side_cluster)):\n",
    "    for col, center_h in enumerate(np.linspace(-2, 2, n_side_cluster)):\n",
    "        step = row + col\n",
    "        new_X, new_y = make_gaussian_quantiles(mean=(center_v, center_h), cov=float(n_side_cluster) ** -2,\n",
    "                                               n_samples=n_blob_sample, n_features=2, n_classes=2, random_state=step)\n",
    "        new_y = np.mod(new_y + step, n_class)\n",
    "        if step == 0:\n",
    "            X, y = new_X, new_y\n",
    "        else:\n",
    "            X, y = np.concatenate((X, new_X)), np.concatenate((y, new_y))\n",
    "X, y = X.astype(np.float32), y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "sk_clf = MLPClassifier(hidden_layer_sizes=[30 for _ in range(5)], activation='relu', solver='adam', alpha=.0001, batch_size=batch_size,\n",
    "                       learning_rate_init=.001, max_iter=n_epoch, random_state=0, tol=1e-5)\n",
    "sk_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "keras_clf = Sequential()\n",
    "for i in range(5):\n",
    "    keras_clf.add(Dense(output_dim=30, input_dim=2 if i == 0 else None))\n",
    "    keras_clf.add(Activation('relu'))\n",
    "keras_clf.add(Dense(output_dim=n_class))\n",
    "keras_clf.add(Activation('softmax'))\n",
    "keras_clf.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "keras_clf.fit(X, to_categorical(y, nb_classes=n_class), batch_size=batch_size, nb_epoch=n_epoch, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets.tuple_dataset import TupleDataset\n",
    "from chainer.iterators import SerialIterator\n",
    "from chainer.training import StandardUpdater, Trainer, extensions\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__(\n",
    "            l1 = L.Linear(2, 30),\n",
    "            l2 = L.Linear(30, 30),\n",
    "            l3 = L.Linear(30, 30),\n",
    "            l4 = L.Linear(30, 30),\n",
    "            l5 = L.Linear(30, 30),\n",
    "            l6 = L.Linear(30, 3)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = F.relu(self.l2(h))\n",
    "        h = F.relu(self.l3(h))\n",
    "        h = F.relu(self.l4(h))\n",
    "        h = F.relu(self.l5(h))\n",
    "        return self.l6(h)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_data = TupleDataset(X, y)\n",
    "ch_clf = L.Classifier(MLP())\n",
    "ch_clf.compute_accuracy = False\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(ch_clf)\n",
    "train_iter = SerialIterator(train_data, batch_size)\n",
    "updater = StandardUpdater(train_iter, optimizer, device=-1)\n",
    "trainer = Trainer(updater, (n_epoch, 'epoch'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "learn = tf.contrib.learn\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "def model(x, y):\n",
    "    y = slim.one_hot_encoding(y, n_class)\n",
    "\n",
    "    net = slim.stack(x, slim.fully_connected, [30 for _ in range(5)], activation_fn=tf.nn.relu)\n",
    "    logits = slim.fully_connected(net, n_class, activation_fn=None)\n",
    "    loss = slim.losses.softmax_cross_entropy(logits, y)\n",
    "    learning_rate = tf.train.exponential_decay(.1, slim.get_global_step(), 1, .9994, staircase=False)\n",
    "    train_op = slim.optimize_loss(loss, slim.get_global_step(), learning_rate=learning_rate, optimizer='SGD')\n",
    "    prob = slim.softmax(logits)\n",
    "\n",
    "    return {'class': tf.argmax(prob, 1), 'prob': prob}, loss, train_op\n",
    "\n",
    "tf_clf = learn.SKCompat(learn.Estimator(model_fn=model))\n",
    "tf_clf.fit(x=X, y=y, steps=n_step, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "resolution = 200\n",
    "linewidth = min(x_max - x_min, y_max - y_min) / resolution\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution))\n",
    "xx, yy = xx.astype(np.float32), yy.astype(np.float32)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# scikit-learn\n",
    "probas = sk_clf.predict_proba(grid)\n",
    "# Keras\n",
    "# probas = keras_clf.predict_proba(grid, verbose=0)\n",
    "# Chainer\n",
    "# probas = F.softmax(ch_clf.predictor(grid)).data\n",
    "# TensorFlow\n",
    "# probas = tf_clf.predict(grid)['prob']\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.scatter(xx.ravel(), yy.ravel(), c=probas, marker='.', alpha=0.6, linewidths=linewidth)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
