{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マージン最大化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "line_x = [X[:, 0].min(), X[:, 0].max()]\n",
    "\n",
    "def hide_ticks(ax):\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "fig = plt.figure(figsize=(9, 3))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y)\n",
    "hide_ticks(ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax2.plot(line_x, [-0.7, 4.3], c='r')\n",
    "ax2.plot(line_x, [3.2, 2.2], c='g')\n",
    "hide_ticks(ax2)\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.scatter(X[:, 0], X[:, 1], c=y)\n",
    "model = SVC(kernel='linear', random_state=0).fit(X, y)\n",
    "coef = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "ax3.plot(line_x, -(np.multiply(line_x, coef[0]) + intercept) / coef[1], c='b')\n",
    "hide_ticks(ax3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "赤や緑の決定境界(直線)でわけるより、青の決定境界でわけておくほうが未知のデータに対する汎化性能は高くなりそう。決定境界から最も近いサンプルまでの距離をマージンと呼び、これを最大化するのがサポートベクターマシン。そのためサポートベクターマシンはマージン最大化器とも呼ばれる。\n",
    "\n",
    "仕組みは、損失関数にヒンジ関数(Hinge Function)を利用した線形分類。\n",
    "\n",
    "純粋なサポートベクターマシンは上のサンプルのような完全線形分類可能な問題(ハードマージン)しか解けないが、現実の問題に完全線形分類可能なものはまずない。そこで、コストペナルティ(C)を導入して誤分類を許容する(ソフトマージン)ようにしている。これは、実質的にサポートベクターマシンの正則化項にあたる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カーネルトリック\n",
    "\n",
    "カーネルトリックでは、指定したカーネル関数を用いてデータを高次元空間に写像し、線形の境界面で分類可能にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X, y = make_circles(n_samples=200, noise=.15, factor=.1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, elev=10, azim=-45)\n",
    "ax = fig.gca(projection='3d')\n",
    "dim_x = X[:, 0]\n",
    "dim_y = X[:, 1]\n",
    "dim_z = np.exp(-dim_x ** 2 - dim_y ** 2)\n",
    "ax.scatter(dim_x, dim_y, dim_z, c=y)\n",
    "\n",
    "x_min, x_max = dim_x.min() - .1, dim_x.max() + .1\n",
    "y_min, y_max = dim_y.min() - .1, dim_y.max() + .1\n",
    "z_min, z_max = dim_z.min() - .1, dim_z.max() + .1\n",
    "\n",
    "resolution = min(x_max - x_min, y_max - y_min) / 80\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution), np.arange(y_min, y_max, resolution))\n",
    "zz = np.zeros(xx.shape)\n",
    "zz.fill(.7)\n",
    "ax.scatter(xx, yy, zz, c='green', marker='.', alpha=0.4, linewidth=resolution)\n",
    "\n",
    "ax.set_xlabel('feature0')\n",
    "ax.set_ylabel('feature1')\n",
    "ax.set_zlabel('new dimension')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_zticks(())\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_zlim(z_min, z_max)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
