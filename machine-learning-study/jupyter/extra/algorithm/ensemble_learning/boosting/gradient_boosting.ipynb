{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配ブースティング(Gradient Boosting)\n",
    "\n",
    "## 勾配ブースティングとは\n",
    "\n",
    "- 勾配降下法を用いたブースティング\n",
    "- 直前の学習器の誤差関数の(負の)勾配が次の学習器の予測する値になる\n",
    "- 予測は全ての学習器の合計を用いる\n",
    "- 勾配ブースティングを改良し、C++で実装した高速に動作するxgboostが流行\n",
    "\n",
    "## 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_gaussian_quantiles, make_circles, make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# データ準備\n",
    "np.random.seed(0)\n",
    "\n",
    "X1, y1 = make_gaussian_quantiles(cov=1.5, n_samples=200, n_features=2, n_classes=2, random_state=0)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(4, 4), cov=1.5, n_samples=200, n_features=2, n_classes=2, random_state=0)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "gaussian_quantiles = (X, y)\n",
    "circles = make_circles(noise=.2, factor=.5, random_state=0)\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1)\n",
    "X += 2 * np.random.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "datasets = [gaussian_quantiles, circles, linearly_separable]\n",
    "rows = len(datasets)\n",
    "\n",
    "# 学習器作成\n",
    "names = [\"Decision Tree\", \"Gradient Boosted Regression Trees\"]\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=1, random_state=0)\n",
    "]\n",
    "cols = len(classifiers) + 1\n",
    "\n",
    "# 可視化\n",
    "ax_size = 3\n",
    "resolution = 300\n",
    "cmap = 'bwr'\n",
    "\n",
    "plt.figure(figsize=(ax_size * cols, ax_size * rows))\n",
    "\n",
    "for row in range(rows):\n",
    "    X, y = datasets[row]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=0)\n",
    "\n",
    "    margin = .5\n",
    "    x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\n",
    "    y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution))\n",
    "\n",
    "    for col in range(cols):\n",
    "        ax = plt.subplot(rows, cols, row * cols + col + 1)\n",
    "\n",
    "        if row == 0:\n",
    "            ax.set_title('Input data' if col == 0 else names[col - 1])\n",
    "        if col > 0:\n",
    "            clf = classifiers[col - 1]\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            mesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "            if hasattr(clf, 'decision_function'):\n",
    "                Z = clf.decision_function(mesh)\n",
    "            else:\n",
    "                Z = clf.predict_proba(mesh)[:, 1]\n",
    "            Z.shape = xx.shape\n",
    "            ax.pcolormesh(xx, yy, Z, cmap=cmap, alpha=.5)\n",
    "\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap)\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, alpha=.6)\n",
    "\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# データ準備\n",
    "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "y = (np.sin(X) + np.sin(X * 6) + np.random.normal(0, .4, size=X.shape)).ravel()\n",
    "\n",
    "# 学習器作成\n",
    "tree = DecisionTreeRegressor(max_depth=4)\n",
    "gb = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=4, random_state=0, loss='ls')\n",
    "\n",
    "tree.fit(X, y)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# 可視化\n",
    "margin = .5\n",
    "x = np.linspace(X.min() - margin, X.max() + margin, 200)[:, np.newaxis]\n",
    "y_tree = tree.predict(x)\n",
    "y_gb = gb.predict(x)\n",
    "\n",
    "plt.plot(x, y_tree, c=\"r\", label=\"Decision Tree\", linewidth=2)\n",
    "plt.plot(x, y_gb, c=\"b\", label=\"Gradient Tree Boosting\", linewidth=2)\n",
    "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.ylim(min(y.min(), y_tree.min(), y_gb.min()) - margin, max(y.max(), y_tree.max(), y_gb.max()) + margin)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
