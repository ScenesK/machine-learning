{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差最小化と最適化\n",
    "\n",
    "機械学習(特に教師あり学習)の目的は、パラメーターを調整して予測値 $\\hat{y}$ を正解 $y$ に近づけること(尤度最大化)や予測値 $\\hat{y}$ と正解 $y$ との差を小さくすること(誤差最小化)。尤度を最大化するのが計算的に難しい場合もあり、誤差最小化の問題として解いてパラメーターを求めることが多い。この尤度最大化や誤差最小化でパラメーターを求めることを最適化という。\n",
    "\n",
    "誤差を単純に正解と予測の差$\\left(y-\\hat{y}\\right)$とすると、負の値を取りうるので最小化できない。正解 $y$ と予測値 $\\hat{y}$ から誤差を求める方法はいくつかある。この誤差(Error)を求める関数のことをコスト関数(Cost function)・損失関数(Loss function)・目的関数(Objective function)などと呼ぶ。(厳密にはそれぞれ違うものを指している場合もある)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均二乗誤差(Mean Squared Error, MSE)<a name=\"mse\"></a>\n",
    "\n",
    "回帰問題の誤差として最も使われる関数。正解 $y$ に含まれるノイズがガウス分布に従う場合に正しい選択だが、世の中の大抵のノイズはガウス分布に従うのでほとんどの場面で使える。\n",
    "\n",
    "### 定義<a name=\"mse_definition\"></a>\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "    MSE=\\frac{1}{n}\\sum_{i=1}^n{\\left(y_i-\\hat{y}_i\\right)^2} \\nonumber\n",
    "\\end{eqnarray}$\n",
    "\n",
    "pythonのコードで表すと\n",
    "\n",
    "> ```python\n",
    "> # n: サンプル(学習)データの数\n",
    "> # y: 正解データ(i番目のサンプルの正解がy[i])\n",
    "> # y_hat: 予測値(i番目のサンプルの予測値がy_hat[i])\n",
    "> mse = np.power(y - y_hat, 2).sum() / n\n",
    "> ```\n",
    "\n",
    "### 特徴<a name=\"mse_character\"></a>\n",
    "\n",
    "差を二乗しているので、正解と予測値の差が最も小さいところが最小誤差になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エントロピー<a name=\"entropy\"></a>\n",
    "\n",
    "情報の乱雑さ・不純さを表す指標。決定木で不純度を表すのに使われる。\n",
    "\n",
    "### 定義<a name=\"entropy_definition\"></a>\n",
    "\n",
    "サンプルに含まれるクラスの数を $c$、クラス $i$ に属するサンプルの割合を $P_i$ とすると、以下で定義される。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "    Entropy=-\\sum_{i=1}^cP_ilogP_i \\nonumber\n",
    "\\end{eqnarray}$\n",
    "\n",
    "### 特徴<a name=\"entropy_character\"></a>\n",
    "\n",
    "二値分類の場合、$P_0=P_1=0.5$ でエントロピーが最大になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "def entropy(p):\n",
    "    return - p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "x = np.linspace(0. + eps, 1. - eps, 50)\n",
    "y = entropy(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.title('Entropy')\n",
    "plt.xlabel('$P_1$')\n",
    "plt.ylabel('Entropy')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差エントロピー誤差(Cross-entropy Error)<a name=\"cross-entropy\"></a>\n",
    "\n",
    "分類問題の誤差としてよく使われる関数。分類の問題を確率回帰の問題に捉え直して誤差を計算している。\n",
    "\n",
    "### 定義<a name=\"cross-entropy_definition\"></a>\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "    CrossEntropy=\\sum_{i=1}^n\\left[-y_ilog\\left(\\hat{y}_i\\right)-\\left(1-y_i\\right)log\\left(1-\\hat{y}_i\\right)\\right] \\nonumber\n",
    "\\end{eqnarray}$\n",
    "\n",
    "pythonのコードで表すと\n",
    "\n",
    "> ```python\n",
    "> # 二値分類の場合、正解ラベルは0か1で表現\n",
    "> # 予測は正解ラベル1のクラスに属する確率で表現\n",
    "> # 多クラス分類問題の正解ラベルはone-hot encodingで表現\n",
    "> # 例えば正解ラベルが5クラスあって、あるサンプルの正解がクラス3なら[0, 0, 1, 0, 0]を渡す\n",
    "> # 予測は各クラスへの所属確率なので、それぞれ0~1の範囲の値を取り、全クラスの予測を合計すると1になる\n",
    "> # 例：[0.01, 0.04, 0.9, 0.03, 0.02]\n",
    "> cross_entropy = (- y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)) # 2クラス\n",
    "> cross_entropy = (- y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).sum(axis=1) # 多クラス\n",
    "> ```\n",
    "\n",
    "### 特徴<a name=\"cross-entropy_character\"></a>\n",
    "\n",
    "正解に近いと誤差が0に近くなり、遠いと誤差が無限大に近づいていく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = 1e-8\n",
    "y_hat = np.linspace(eps, 1 - eps, 200)\n",
    "cost_pos = - np.log(y_hat)\n",
    "cost_neg = - np.log(1 - y_hat)\n",
    "\n",
    "plt.plot(y_hat, cost_pos, label='y = 1')\n",
    "plt.plot(y_hat, cost_neg, label='y = 0')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('cost')\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配降下法(Gradient Descent)<a name=\"gradient_descent\"></a>\n",
    "\n",
    "勾配降下法は最適化手法の1つで、コスト関数をパラメーター $\\theta$ についての関数 $J(\\theta)$ とみなし、$J(\\theta)$ の勾配を下っていく( $J(\\theta)$ の傾きと逆に進む)ことでコスト関数の最小値を目指す。(図は説明のためのもので、計算式は実際とは異なる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J(theta):\n",
    "    return theta ** 2\n",
    "\n",
    "def J_grad(theta):\n",
    "    return theta * 2\n",
    "\n",
    "def gradient_descent(theta, alpha, n_step):\n",
    "    thetas = np.zeros(n_step)\n",
    "    costs = np.zeros(n_step)\n",
    "    grads = np.zeros((n_step, 2))\n",
    "\n",
    "    for step in range(n_step):\n",
    "        thetas[step] = theta\n",
    "        costs[step] = J(theta)\n",
    "        grad = alpha * J_grad(theta)\n",
    "        theta -= grad\n",
    "        grads[step] = [- grad, J(theta) - J(thetas[step])]\n",
    "\n",
    "    return (thetas, costs, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = .3 # 学習率\n",
    "theta = 9 # パラメーターの初期値\n",
    "n_step = 3 # パラメーターの更新回数\n",
    "\n",
    "plt.title('Concept of gradient descent 1D')\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = J(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "thetas, costs, grads = gradient_descent(theta, alpha, n_step)\n",
    "plt.quiver(thetas, costs, grads[:, 0], grads[:, 1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('cost')\n",
    "plt.xlim(x.min() - 5, x.max() + 5)\n",
    "plt.ylim(y.min() - 5, y.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "alpha = .3\n",
    "theta = np.array([9, 9])\n",
    "n_step = 3\n",
    "\n",
    "def J_2d(theta):\n",
    "    return np.power(theta, 2).sum()\n",
    "\n",
    "def J_grad_2d(theta):\n",
    "    return theta * 2\n",
    "\n",
    "def gradient_descent_2d(theta, alpha, n_step):\n",
    "    thetas = np.zeros(tuple([n_step]) + theta.shape)\n",
    "    costs = np.zeros(n_step)\n",
    "    grads = np.zeros((n_step, len(theta) + 1))\n",
    "    \n",
    "    for step in range(n_step):\n",
    "        thetas[step] = theta\n",
    "        costs[step] = J_2d(theta)\n",
    "        grad = alpha * J_grad_2d(theta)\n",
    "        theta = theta - grad\n",
    "        grads[step] = np.concatenate((- grad, np.array([J_2d(theta) - J_2d(thetas[step])])))\n",
    "    \n",
    "    return (thetas, costs, grads)\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = np.linspace(-10, 10, 30)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "z = np.power(grid, 2).sum(axis=1).reshape(xx.shape)\n",
    "\n",
    "x_min, x_max = x.min(), x.max()\n",
    "y_min, y_max = y.min(), y.max()\n",
    "z_min, z_max = z.min() - 20, z.max()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, elev=25, azim=-75)\n",
    "\n",
    "ax.set_title('Concept of gradient descent 2D')\n",
    "\n",
    "ax.plot_wireframe(xx, yy, z, rstride=2, cstride=2, alpha=.3)\n",
    "ax.contour(xx, yy, z, zdir='z', offset=z_min)\n",
    "\n",
    "thetas, costs, grads = gradient_descent_2d(theta, alpha, n_step)\n",
    "\n",
    "for qx, qy, qz, qu, qv, qw in zip(thetas[:, 0], thetas[:, 1], costs, grads[:, 0], grads[:, 1], grads[:, 2]):\n",
    "    length = np.sqrt(np.power([qu, qv, qw], 2).sum())\n",
    "    ax.quiver(qx, qy, qz, qu, qv, qw, pivot='tail', length=length, arrow_length_ratio=min(.3, 3.0 / length), color='red')\n",
    "\n",
    "x_flat = np.append(thetas[:, 0], thetas[-1, 0] + grads[-1, 0])\n",
    "y_flat = np.append(thetas[:, 1], thetas[-1, 1] + grads[-1, 1])\n",
    "z_flat = [z_min] * (n_step + 1)\n",
    "ax.plot(x_flat, y_flat, z_flat, marker='x', color='red')\n",
    "\n",
    "ax.set_xlabel('$\\\\theta_1$')\n",
    "ax.set_ylabel('$\\\\theta_2$')\n",
    "ax.set_zlabel('cost')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_zlim(z_min, z_max)\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_zticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率 $\\alpha$ は大きすぎるとコストが発散してしまい、小さすぎると収束に時間がかかりすぎる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw(ax, theta, alpha, n_step):\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    thetas, costs, grads = gradient_descent(theta, alpha, n_step)\n",
    "    ax.quiver(thetas, costs, grads[:, 0], grads[:, 1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "\n",
    "    ax.set_xlim(x.min() - 5, x.max() + 5)\n",
    "    ax.set_ylim(y.min() - 5, y.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = J(x)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Too large')\n",
    "draw(ax, 4, 1.1, 5)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Too small')\n",
    "draw(ax, 9, .03, 10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
