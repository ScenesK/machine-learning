{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ストレージ・メモリ・実行速度の向上\n",
    "\n",
    "- 利用する特徴を減らすことで、ストレージ・メモリの使用量を減らしたり、モデルを早く動かせる\n",
    "- 正則化が適用できないモデルでは過学習の防止に役立つが、正則化が適用できるモデルでは過学習防止には正則化を用いる\n",
    "\n",
    "## 特徴選択(feature selection)\n",
    "\n",
    "全ての特徴から任意の個数の特徴を選び出す。選び出された特徴自体は元のまま。\n",
    "\n",
    "### Lasso回帰\n",
    "\n",
    "[Lasso回帰](/notebooks/jupyter/basic/improvement/regularization/lasso_regression.ipynb)は特徴選択に利用可能。\n",
    "\n",
    "### Recursive Feature Elimination\n",
    "\n",
    "重みの小さいものから順に削除していき、特徴の重要度をランク付けする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "loader = load_breast_cancer()\n",
    "X, y = loader.data, loader.target\n",
    "model = Perceptron()\n",
    "selector = RFE(estimator=model, n_features_to_select=5, step=1)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print('Original Feature: {origin}, Selected Feature: {selected}'.format(origin=X.shape[1], selected=X_new.shape[1]))\n",
    "ranking = selector.ranking_\n",
    "rank_order = np.argsort(ranking)\n",
    "for i, rank_id in enumerate(rank_order):\n",
    "    print('Feature Importance Rank {rank:>2}: Feature {feature}'.format(rank=ranking[rank_id], feature=rank_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト\n",
    "\n",
    "[ランダムフォレストを利用して特徴の重要度を導く方法](random_forest_feature_importance.ipynb)もある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元削減(Dimensionality Reduction)\n",
    "\n",
    "多次元の特徴を、できるだけ情報を保持したまま低次元の特徴に落とす。得られた特徴は元の特徴とは異なる。\n",
    "\n",
    "### 主成分分析(Principal Component Analysis, PCA)\n",
    "\n",
    "- 多次元の相関のある特徴を、低次元の相関の少ない特徴に落とす手法\n",
    "- データのスケールに対して敏感なので、実施前に各特徴を標準化しておかなければならない\n",
    "\n",
    "#### 使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# データ準備\n",
    "np.random.seed(0)\n",
    "\n",
    "X, _ = make_classification(n_samples=300, n_features=3, n_informative=2, n_redundant=1, n_clusters_per_class=1, random_state=0)\n",
    "X += np.random.normal(scale=.2, size=X.shape)\n",
    "x, y, z = X[:, 0], X[:, 1], X[:, 2]\n",
    "\n",
    "# 主成分分析\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new = pca.transform(X)\n",
    "\n",
    "# 可視化\n",
    "x_pca, y_pca = X_new[:, 0], X_new[:, 1]\n",
    "margin = .5\n",
    "x_pca_min = y_pca_min = min(x_pca.min(), y_pca.min()) - margin\n",
    "x_pca_max = y_pca_max = max(x_pca.max(), y_pca.max()) + margin\n",
    "xx_pca, yy_pca = np.meshgrid([x_pca_min, x_pca_max], [y_pca_min, y_pca_max])\n",
    "rect_pca = np.c_[xx_pca.ravel(), yy_pca.ravel()]\n",
    "rect = pca.inverse_transform(rect_pca)\n",
    "x_rect, y_rect, z_rect = rect[:, 0], rect[:, 1], rect[:, 2]\n",
    "x_rect.shape = (2, 2)\n",
    "y_rect.shape = (2, 2)\n",
    "z_rect.shape = (2, 2)\n",
    "\n",
    "def plot(fig_id, elev, azim):\n",
    "    fig = plt.figure(fig_id)\n",
    "    ax = Axes3D(fig, elev=elev, azim=azim)\n",
    "    ax.scatter(x, y, z, c='blue', marker='+')\n",
    "    ax.plot_surface(x_rect, y_rect, z_rect, color='red', alpha=.6)\n",
    "    ax.set_xticklabels(())\n",
    "    ax.set_yticklabels(())\n",
    "    ax.set_zticklabels(())\n",
    "\n",
    "plot(0, -55, 120)\n",
    "plot(1, -145, 120)\n",
    "\n",
    "plt.figure(2, figsize=(5, 5))\n",
    "plt.scatter(x_pca, y_pca, c='blue', marker='+')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_components(model):\n",
    "    components = model.components_\n",
    "    n_components = components.shape[0]\n",
    "    n_features = components.shape[1]\n",
    "    features = np.arange(n_features)\n",
    "    x_min, x_max = -1, n_features\n",
    "    y_min, y_max = components.min() - .05, components.max() + .05\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 2 * n_components))\n",
    "\n",
    "    for i in range(n_components):\n",
    "        fig.add_subplot(n_components, 1, i + 1)\n",
    "        plt.title('Component {n}'.format(n=i))\n",
    "        plt.bar(features, components[i, :], align='center')\n",
    "\n",
    "        if i < n_components - 1:\n",
    "            plt.xticks(())\n",
    "        else:\n",
    "            labels = ['feature {n}'.format(n=n) for n in features]\n",
    "            plt.xticks(features, labels, rotation='vertical')\n",
    "\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0, hspace=.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# データ準備\n",
    "loader = load_diabetes()\n",
    "X = loader.data\n",
    "\n",
    "# 主成分分析\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# 各成分の可視化\n",
    "print('Explained variance ratio: ', ['{v:.3f}'.format(v=v) for v in pca.explained_variance_ratio_])\n",
    "\n",
    "for n in range(X.shape[1], 0, -1):\n",
    "    print('New dimensions: {n}, Left variance: {v:.3f}'.format(n=n, v=pca.explained_variance_ratio_[:n].sum()))\n",
    "\n",
    "draw_components(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形判別分析(Linear Discriminant Analysis)\n",
    "\n",
    "- 教師あり次元削減に利用可能な手法\n",
    "- 訓練データの異なるラベル同士が最も離れるように低次元の特徴に落とす\n",
    "\n",
    "#### 使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# データ準備\n",
    "X, y = make_blobs(centers=([[1, 0]]), cluster_std=.35, random_state=0)\n",
    "X = np.concatenate((X, -1 * X), axis=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = np.concatenate((y, np.ones(len(y))), axis=0)\n",
    "\n",
    "# 線形判別分析と主成分分析の比較\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "titles = ['Original Data', 'LDA', 'PCA']\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i == 0:\n",
    "        xx, yy = X[:, 0], X[:, 1]\n",
    "    elif i == 1:\n",
    "        xx = LinearDiscriminantAnalysis(n_components=1).fit_transform(X, y)\n",
    "        yy = np.zeros_like(xx)\n",
    "    else:\n",
    "        xx = PCA(n_components=1).fit_transform(X)\n",
    "        yy = np.zeros_like(xx)\n",
    "        yy[y == 0] = .0005\n",
    "        yy[y == 1] = -.0005\n",
    "    ax.set_title(titles[i])\n",
    "    ax.scatter(xx, yy, c=y, cmap='bwr', marker='x')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ準備\n",
    "X, y = loader.data, loader.target\n",
    "\n",
    "# 線形判別分析\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 各成分の可視化\n",
    "print('Explained variance ratio: ', ['{v:.3f}'.format(v=v) for v in lda.explained_variance_ratio_])\n",
    "\n",
    "for n in range(X.shape[1], 0, -1):\n",
    "    print('New dimensions: {n}, Left variance: {v:.3f}'.format(n=n, v=lda.explained_variance_ratio_[:n].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 潜在的ディリクレ配分法(Latent Dirichlet Allocation, LDA)\n",
    "\n",
    "- 文書の特徴抽出・次元削減に利用可能な手法\n",
    "- 文書に含まれる単語から、文書のトピック(話題・カテゴリ)を確率的に推測する\n",
    "\n",
    "#### 使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# データ準備\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "datasets = fetch_20newsgroups(shuffle=True, random_state=0, remove=('headers', 'footers', 'quotes'))\n",
    "data = datasets.data\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=2 ** 10)\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=5, learning_method='online',\n",
    "                                learning_offset=50., n_jobs=-1, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各トピックの可視化\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print('Topic {n}: {topic}'.format(n=idx, topic=[feature_names[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文書への適用\n",
    "import numpy as np\n",
    "\n",
    "doc_id = 0\n",
    "topics = lda.transform(X[0])[0]\n",
    "\n",
    "print('Document {i}'.format(i=doc_id))\n",
    "for i, p in enumerate(topics):\n",
    "    print('Topic {i}: {p:.3f}'.format(i=i, p=p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
